{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPHW1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOe/uIAURb7o89d2eNApcrJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliKarimiENT/NLP_HW1/blob/main/NLPHW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install libraries"
      ],
      "metadata": {
        "id": "Amyp3uXbX3Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install hazm"
      ],
      "metadata": {
        "id": "JPoyMlwZYQDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "mrw72Kcty3LB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from os import remove\n",
        "from hazm import word_tokenize\n",
        "from hazm import Stemmer\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing import text\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "rSgsa9JiypuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading txt file and getting verses\n",
        "\n",
        "Here we need to do some processes"
      ],
      "metadata": {
        "id": "58KGNeYk_1zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with this function we check that it contains number in the text or not\n",
        "def hasNumbers(inputString) :\n",
        "  for character in inputString :\n",
        "    if character.isdigit() :\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def hasDash(inputString) :\n",
        "  for character in inputString :\n",
        "    if character == '-' :\n",
        "      return True\n",
        "  return False \n",
        "\n",
        "\n",
        "with open('/content/ShamsDaftar6.doc_filename=ShamsDaftar6.txt') as f:\n",
        "  dataSet = f.readlines()\n",
        "\n",
        "verses = []\n",
        "for line in dataSet :\n",
        "  if line != ' ' and line != '\\n' and hasNumbers(line) == False  and hasDash(line)==False:\n",
        "    if dataSet.index(line) != 0 :\n",
        "      verses.append(line.replace('\\t',' '))\n",
        "\n",
        "\n",
        "# for i in range(20):\n",
        "#   print(verses[i])\n"
      ],
      "metadata": {
        "id": "Th7fYgmq-xCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/stopwords.txt') as f:\n",
        "  stops = f.readlines()\n",
        "stopWords = []\n",
        "for stop in stops :\n",
        "  stopWords.append(re.sub('\\n','',stop.replace('\\ufeff','')))\n",
        "print(stopWords)"
      ],
      "metadata": {
        "id": "jMHFSQptcwpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to use **Hazm** **Library** to tokenize each sentence"
      ],
      "metadata": {
        "id": "IZQauXWVYkjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "concept_words = []\n",
        "for verse in verses:\n",
        "  # now we must add all words in the list to words list\n",
        "  # for word in word_tokenize(verse):\n",
        "  for word in verse.split():\n",
        "    concept_words.append(word)"
      ],
      "metadata": {
        "id": "WypH9rQGY3C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(concept_words)"
      ],
      "metadata": {
        "id": "J7_ya6UJit7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove stop words from verses"
      ],
      "metadata": {
        "id": "KBxPzT78XTC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for removing stop words from our words list we need to this process\n",
        "corpus  = [] #concept words list is a list that contains list of words without stopWords\n",
        "stemmer = Stemmer()\n",
        "for word in concept_words :\n",
        "  wd = stemmer.stem(word)\n",
        "  if word not in stopWords:\n",
        "    if wd not in stopWords:\n",
        "        corpus.append(wd)\n",
        "    # else:\n",
        "    #     if word != wd :\n",
        "    #       corpus.append(word)\n",
        "words = []\n",
        "for word in corpus:\n",
        "  words.append(word)\n",
        "corpus = set(corpus)\n",
        "print(corpus)"
      ],
      "metadata": {
        "id": "aqCmQpuyXOvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(corpus))"
      ],
      "metadata": {
        "id": "AEBwrz3KMugq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Tokenize** corpus vocabulary\n",
        "\n",
        "Import keras and use it "
      ],
      "metadata": {
        "id": "s0zda83xbXzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now,we want a list of our sentences as list of words:"
      ],
      "metadata": {
        "id": "ypgZoBFPGso8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "# wordToIndex = tokenizer.word_index\n",
        "# indexToWord = {v:k for k,v in wordToIndex.items()}\n",
        "# print(wordToIndex)\n",
        "wordToIndex = {}\n",
        "indxToWord = {}\n",
        "\n",
        "\n",
        "vocab_size = len(corpus)\n",
        "for i,word in enumerate(corpus):\n",
        "    wordToIndex[word] = i\n",
        "    indxToWord[i] = word \n",
        "embed_size = 100\n",
        "\n",
        "# print(corpus)\n",
        "# windows = [[wordToIndex[w] for w in text.text_to_word_sequence(doc)] for doc in corpus]\n",
        "print(vocab_size)\n",
        "print(wordToIndex.items())\n",
        "# Now each unique word from the corpus is a part of our vocabulary now with a unique numeric identifier\n",
        "\n"
      ],
      "metadata": {
        "id": "xNC5votYaQRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "for verse in verses:\n",
        "  sentence = []\n",
        "  for word in verse.split():\n",
        "    wd = stemmer.stem(word)\n",
        "    if word not in stopWords:\n",
        "      if wd not in stopWords:\n",
        "        if wd not in sentence:\n",
        "          sentence.append(wd) \n",
        "      # else:\n",
        "      #   if word != wd :\n",
        "      #     if word not in sentence :\n",
        "      #       sentence.append(word)\n",
        "        # sentence.append(stemmer.stem(word))\n",
        "  sentences.append(sentence)\n",
        "  \n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "hazn_taUGrsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will train our data"
      ],
      "metadata": {
        "id": "lrCZ5VzoxrAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.data.ops.dataset_ops import AUTOTUNE\n",
        "data = []\n",
        "WINDOW_SIZE = 2\n",
        "for sentence in sentences :\n",
        "  for word_index , word in enumerate(sentence) :\n",
        "    for nb_word in sentence[max(word_index - WINDOW_SIZE,0) : min(word_index + WINDOW_SIZE , len(sentences)) + 1] :\n",
        "      if nb_word != word :\n",
        "        data.append([word,nb_word])\n",
        "# dataset = tf.data.T\n",
        "data = np.asarray(data)\n",
        "# tensor1 = tf.data.Dataset.from_tensor_slices(data)\n",
        "# print(type(tensor1))\n",
        "# tensor1 = tensor1.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "# print(tensor1)\n"
      ],
      "metadata": {
        "id": "9Ha8J9GPxwId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets define our **One Hot Function**\n",
        "\n",
        "This function will convert numbers to one hot vectors"
      ],
      "metadata": {
        "id": "99tKnLLNzBCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_one_hot(data_point_index,vocab_size):\n",
        "  temp = np.zeros(vocab_size)\n",
        "  temp[data_point_index] = 1\n",
        "  return temp"
      ],
      "metadata": {
        "id": "8sm92ZTpzN7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data))"
      ],
      "metadata": {
        "id": "-AYeWFBHIwra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initail x and y train values \n",
        "\n",
        "x train is a list of our input words in one hot form\n",
        "\n",
        "y train is a list of out output words in one hot form\n"
      ],
      "metadata": {
        "id": "MqveFxSL0pLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(tensor1.as_numpy_iterator())\n",
        "# list(tensor1.as_numpy_iterator())"
      ],
      "metadata": {
        "id": "aXQPxCZc58i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "\n",
        "for data_word in data:\n",
        "  \n",
        "  x_train.append(tf.one_hot(wordToIndex[data_word[0]],vocab_size))\n",
        "  y_train.append(tf.one_hot(wordToIndex[data_word[1]],vocab_size))\n",
        "\n",
        "#convert them to numpy arrays\n",
        "x_train = np.asarray(x_train)\n",
        "y_train = np.asarray(y_train)\n"
      ],
      "metadata": {
        "id": "iLKFcHnj04Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both x_traing and y_train have shape :"
      ],
      "metadata": {
        "id": "ktnQ3wO4HhrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape,y_train.shape)"
      ],
      "metadata": {
        "id": "cuC-BMDZHx6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to make **Tensorflow model**\n"
      ],
      "metadata": {
        "id": "vm9iiwchH3DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "x = tf.compat.v1.placeholder(tf.float32, shape=(None, vocab_size),name=None)\n",
        "y_label = tf.compat.v1.placeholder(tf.float32, shape=(None, vocab_size),name=None)\n",
        "\n",
        "print(x)\n",
        "print(y_label)"
      ],
      "metadata": {
        "id": "AeXS5czFH-xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert training data into **Embedded representation**"
      ],
      "metadata": {
        "id": "Kxf5pujEPWL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 5\n",
        "W1 = tf.Variable(tf.random.normal([vocab_size,EMBEDDING_DIM]))\n",
        "b1 = tf.Variable(tf.random.normal([EMBEDDING_DIM])) #bias\n",
        "\n",
        "hidden_representation = tf.add(tf.matmul(x,W1),b1)\n",
        "\n",
        "print(W1)\n",
        "print(b1)\n",
        "print(hidden_representation)"
      ],
      "metadata": {
        "id": "XMOuVB-dPhDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making a prediction about neighbours with softmax"
      ],
      "metadata": {
        "id": "O9RZFwvkQ9uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W2 = tf.Variable(tf.random.normal([EMBEDDING_DIM,vocab_size]))\n",
        "b2 = tf.Variable(tf.random.normal([vocab_size]))\n",
        "\n",
        "prediction = tf.nn.softmax(tf.add(tf.matmul(hidden_representation,W2),b2))\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "u-lMr9OiRILv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go to train it"
      ],
      "metadata": {
        "id": "QpnePSMLSnPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define session \n",
        "session = tf.compat.v1.Session()\n",
        "\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "session.run(init)\n",
        "\n",
        "# define the loss function\n",
        "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.math.log(prediction),[1]))\n",
        "\n",
        "# define the training step\n",
        "train_step = tf.compat.v1.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\n",
        "\n",
        "n_iters = 10\n",
        "\n",
        "# train for n_iter iterations\n",
        "\n",
        "print(session.run(W1))\n",
        "\n",
        "# for _ in range(n_iters) :\n",
        "  # session.run(train_step,feed_dict={x: x_train, y_label: y_train})\n",
        "\n",
        "  # print('loss is : ', session.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))"
      ],
      "metadata": {
        "id": "MM_xhIQuSrT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding bias"
      ],
      "metadata": {
        "id": "tZw7z3arlQ7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = session.run(W1+b1)\n",
        "# print(vectors)\n",
        "\n",
        "print(vectors[wordToIndex['آبنوس']])"
      ],
      "metadata": {
        "id": "QB4PwXMOlufo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_dist(vec1, vec2):\n",
        "    return np.sqrt(np.sum((vec1-vec2)**2))\n",
        "def find_closest(word_index, vectors):\n",
        "    min_dist = 10000 # to act like positive infinity\n",
        "    min_index = -1\n",
        "    dictionary = {}\n",
        "    query_vector = vectors[word_index]\n",
        "    min_indexes = []\n",
        "    for index, vector in enumerate(vectors):\n",
        "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n",
        "            min_dist = euclidean_dist(vector, query_vector)\n",
        "            dictionary[index] = min_dist\n",
        "    print(dictionary)\n",
        "    sorted_by_value = dict(sorted(dictionary.items(),key= lambda item : item[1],reverse=True))\n",
        "    for i in range(15) :\n",
        "      min_indexes.append(sorted_by_value[i][0])\n",
        "    print(min_indexes)\n",
        "    return min_indexes"
      ],
      "metadata": {
        "id": "MIOFBw4xtds2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = wordToIndex['عشق']\n",
        "min_dist = 10000 # to act like positive infinity\n",
        "min_index = -1\n",
        "dictionary = {}\n",
        "query_vector = vectors[word_index]\n",
        "min_indexes = []\n",
        "for index, vector in enumerate(vectors):\n",
        "    if euclidean_dist(vector, query_vector) and not np.array_equal(vector, query_vector):\n",
        "        min_dist = euclidean_dist(vector, query_vector)\n",
        "        dictionary[index] = min_dist\n",
        "sorted_by_value = dict(sorted(dictionary.items(),key= lambda item : item[1]))\n",
        "key_list = list(sorted_by_value.keys())\n",
        "for i in range(15) :\n",
        "  # print(sorted_by_value.get(sorted_by_value[i]))\n",
        "  min_indexes.append(key_list[i])\n",
        "\n",
        "# closest = find_closest(wordToIndex['بابل'], vectors)\n",
        "\n",
        "for close in min_indexes :\n",
        "  print(indxToWord[close])\n"
      ],
      "metadata": {
        "id": "V_r-idqrmfFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "model = TSNE(n_components=2, random_state=0)\n",
        "np.set_printoptions(suppress=True)\n",
        "vectors = model.fit_transform(vectors)"
      ],
      "metadata": {
        "id": "IQpaoaPJnN3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "normalizer = preprocessing.Normalizer()\n",
        "vectors =  normalizer.fit_transform(vectors, 'l2')"
      ],
      "metadata": {
        "id": "Svd-yV6PnmuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "for word in words[1:20]:\n",
        "    print(word, vectors[wordToIndex[word]][1])\n",
        "    ax.annotate(word, (vectors[wordToIndex[word]][0],vectors[wordToIndex[word]][1] ))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NQvVKd6CnqYd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}